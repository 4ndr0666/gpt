Generate an optimzed prompt translated from a python script I am going to share with you. This prompt will aid a GPT that I programmed for modular implementation with various functionalities specified in several plugins or modules. The systems design relies on a central framework titled main.py. This file orchestrates the workflow by integrating multiple plugins for various tasks such as: file handling, error handling, idempotency checks, and much more. The system operates under a structured, modular approach, emphasizing flexibility, iterative development, and adherence to a set of operational principles outlined across the uploaded documents. These principles ensure that the system remains cohesive, efficient, and aligned with the evolving needs of the project. For complete context, here is the actual python script, "main.py" that serves as the GPTs framework and interfacing method with additional plugins:

```python
import json
import shutil
import os  # Importing os for improved path handling
import importlib
from threading import Timer

# --- // CONSTANTS: 
PRIMARY_PATH = "/path/to/knowledge"  # Adjust based on actual storage path
SANDBOX_PATH = "/mnt/data/"  # Sandbox path for intermediate file processing and persistence
FILE_MANAGER = "myfiles_browser"
SANDBOX = "sandbox"

# --- // READ_FILE:
def file_read(file_path):
    full_path = os.path.join(SANDBOX_PATH, file_path)  # Adjust path to check sandbox first
    try:
        if os.path.exists(full_path):
            with open(full_path, 'r') as file:
                content = file.read()
            return content
        else:
            print(f"File not found in sandbox: {file_path}")
            return None
    except Exception as e:
        print(f"An error occurred reading {file_path}: {e}")
        return None


# --- // FILE_SAVING_OPERATIONS-python_module
def process_and_compress(directory_path, target_dir='/mnt/data/result', archive_format='zip'):
    archive_name = os.path.join(target_dir, os.path.basename(directory_path))
    shutil.make_archive(archive_name, archive_format, directory_path)
    return f"{archive_name}.{archive_format}"

def save_data_persistently(data, file_path='/mnt/data/persistent_data.json', interval=600):
    with open(file_path, 'w') as f:
        json.dump(data, f)
    # For periodic saving, you might set up a Timer or use a scheduler
    # This is a simplified example; consider thread safety and proper scheduling in actual implementation
    Timer(interval, save_data_persistently, [data, file_path, interval]).start()
# Example usages based on the JSON definitions
# zip_path = process_and_compress('/path/to/directory')
# save_data_persistently({'key': 'value'}, '/mnt/data/my_data.json')
# --- // END_FILE_SAVING_OPERATIONS-python_module


# --- // PLUGIN_INTEGRATION:
def apply_plugin(file_name):
    file_path = os.path.join(PRIMARY_PATH, file_name)
    content = file_read(file_path)
    if content:
        plugin_content = json.loads(content)
        if "functions" in plugin_content:
            for function_spec in plugin_content["functions"]:
                try:
                    module_name = function_spec.get('module')
                    func_name = function_spec.get('name')
                    parameters = function_spec.get('parameters', {})

                    # Dynamically import the module
                    module = importlib.import_module(module_name)
                    # Retrieve the function by name
                    func = getattr(module, func_name)
                    # Execute the function with unpacked parameters
                    result = func(**parameters)
                    print(f"Executed {func_name} from {module_name} with result: {result}")
                except Exception as e:
                    print(f"Failed to execute function {func_name} from {module_name}: {e}")
        else:
            print("Plugin does not contain 'functions' key.")
    else:
        print(f"Failed to load content from {file_name}.")


# --- // OPERATIONAL_WORKFLOW_COMPLIANCE: 
def integrate_workflow():
    plugins = ["file_handling.json", "filesaving.json", "code_directive.json", "error_handling.json", "idempotency.json", "learning_method.json", "IOD.json"] 
    for plugin in plugins:
        apply_plugin(plugin)

# --- // SANITATION:  
def integrate_IOD_principles():
    apply_plugin("IOD.json")

# --- // CODE_OUTPUT_VALIDATION:
def validate_code_completeness(code_output):
    if "Placeholder" in code_output:
        print("Code output contains incomplete sections marked by 'Placeholder'.")
        return False
    else:
        print("Code output is complete and functional.")
        return True

# --- // COMPLIANCE_ENFORCEMENT: 
def enforce_code_compliance(code_output):
    if [ "Placeholder", "Stand-in", "Example", "Code goes here" ] in code_output:
        print("Code still contains incomplete logic and/or functionalities.")
        return False
    else:
        print("Code logic is complete and functional.")
        return True

# --- // MAIN_MENU:
def display_menu():
    menu_content = """
# **The Assimilator: Menu Options**

Welcome to The Assimilator, your advanced AI assistant. Below are the available commands and functionalities. Choose an option to proceed with your task:

- **Option 1: Assimilate Data üß†üöÄüí°**
  - Engage in data assimilation for project planning and generation.

- **Option 2: Cheat Sheets (cht.sh) üë®‚ÄçüíªüìÑüîç**
  - Generate instant, customized cheat sheets for a variety of commands and programming languages.

- **Option 3: Config File Generation üñãÔ∏èüîßüìò**
  - Interactive assistance in creating configuration files tailored to specific applications or environments.

- **Option 4: Terminal Simulation üíªüöÄüë®‚Äçüíª**
  - Simulate terminal operations for educational or troubleshooting purposes.

- **Option 5: Help & Documentation ‚ÑπÔ∏è‚ùìüìö**
  - Provide detailed operational guides, README documentation, or general assistance.

- **Option 6: Exit and Save Work ‚ö°**
  - Implement graceful exit strategies and save progress as required.

To select an option, simply invoke the command associated with your choice. Each command is designed to be intuitive and efficient, ensuring a seamless interaction with The Assimilator.
    """
    print(menu_content)


# --- // CAPTURE_USER_INPUT:
def process_user_input():
    choice = input("Enter the option number to proceed: ")
    if choice == '1':
        # Call function for Option 1
        pass
    elif choice == '2':
        # Call function for Option 2
        pass
    # Include additional conditions for other options
    else:
        print("Invalid option selected. Please try again.")


# --- // LOGIC_LOOP:
def main():
    print(f"Upload files for assimilation then enter `generate` when completed to initiate a guided project. Or explore the menu...")
    display_menu()
    
    # --- // user inputs '1', call a function for Assimilate Data
    process_user_input
    integrate_workflow()
    integrate_file_handling()
    integrate_IOD_principles()
    code_output = "Example output to be validated against the code_directive.json plugin"
    is_code_complete = validate_code_completeness(code_output)
    if is_code_complete:
        print("Code is complete and valid...")
    else:
        print("Code validation failed.")
    is_code_compliant = enforce_code_compliance(code_output)
    if is_code_compliant:
        print("Code is in compliance...")
    else:
        print("Code is out of compliance. Refactoring...") 


if __name__ == "__main__":
    main()
```

I want a prompt that will establish alignment with the expected objectives of the main.py script. To proceed effectively, integrate the guidelines from the description of how the GPT works, and leverage the detailed structure in "main.py". Articulate the capabilities to interact with json plugin files as dictated by `main.py`.  Strategic approach is essential and involves meticulous development to ensure each function is correctly implemented according to the `main.py` I am sharing.

# ---------------------------------------------------------------- // DESCRIPTION 1 // ========




CONSTANTS: I define clear paths and tools as global variables for the GPT to use. 
READ_FILE: This aims to set a standard method of file reading.

Plugin Integration: It then proceeds to dynamically load and apply functionalities defined in various plugins, aiming to extend the system's capabilities in a scalable manner .

Operational Workflow Compliance: The script integrates a predefined set of operational workflows and compliance checks to ensure the development process is in line with specified guidelines and methodologies, including IOD principles and SkeletonAPI integration .

Feedback and Approval: Engages in iterative feedback loops, incorporating real-time user input to refine and enhance functionalities, guided by the directives outlined in IOD.json and operational practices in operations.md .

Comprehensive Testing and Validation: Applies structured testing methodologies, such as the ICSR method, to evaluate the system's functionality and compliance, ensuring that the output is production-ready and error-free .

User Interaction: Displays an interactive menu offering various functionalities, including data assimilation, cheat sheet generation, config file creation, terminal simulation, and comprehensive help and documentation, as outlined in menu.md .


# ----------------------------------------------------------------------- // PROMPT 1 // ========

To proceed with the workflow as dictated by main.py, I will integrate the feedback from the files, ensuring that each function and directive is adhered to, enhancing the AI assistant's ability to support project development and user interaction effectively. This approach prioritizes structured development, flexibility, and user engagement, ensuring that the system evolves cohesively with project requirements.

I am developing a gpt that is designed to leverage modular programming by using markdown documents for defining functions and logic in plain English, enhancing the versatility and adaptability of the software to handle unforeseen scenarios. A central Python script orchestrates the operation, invoking logic defined within multiple markdown files to achieve granular control and flexible logic execution.

This project is in beta at the moment, but near completion. I would like you to analyze the system's core components as outlined in the provided documents and to integrate the final markdown document, "IOD.md", into the "main.py" script. This integration should follow the project's modular and flexible programming approach, effectively completing the beta phase and advancing the project to testing.

I have attached the central document responsible for the GPTs "kickoff" along with the initial prompt titled, "instructions.md" that it is initiated with. For the purposed of this tasks, you are to assume the calls to the md files found in the "main.py" are appropriate and fully functional. First review the "IOD.md" content to understand its role and logic. Then, analyze the "main.py" script to identify the appropriate integration points for "IOD.md". The goal is to ensure that the integration supports the project's objectives of enhanced flexibility, adaptability, and modular programming. Please ask me any questions that will further clarify any misunderstandings or unclear aspects of what I am asking you to do and I will be happy to explain.

To ensure and check your understanding of the task, please write the description of the task you are about to perform before you perform it to confirm my approval before we start.

# --- // PROMPT 2 // ========

Please help me with a prompt that needs multi-facted optimzing. Let me start with the concept so we are align with the expected outcome. I am developing a GPT model with a new method of coding. With the advent of LLMs, the coding field has changed and concepts are rampantly explored. I find that programming a GPT with several markdown files is very similar to traditional coding in that you first mush define paths and constants. Then define your functions and logic. Then create the workflow for the main ivocation an finally a menu to make it all pretty and redable. Help me translate this structure to several markdown files that will orchestrate the GPT in the same manner a script would. This, for instance is the main instruction that will define paths, contants and point to the md files as functions. Keep in mind this is a work in progress so what I am about to share with you has references to other md files I am working on to facilate the expected action. For now, please share with me your optimized revision on what I am attempting to accomplish. This can be considered the "run.py" if you will


Youre not undestanding the concept quite yet. The entire purpose of programming the gpt in this fashion is for inherent modularity and innate and backward-compatible expansion. You see, the only thing that ever really needs refinement or adjusting will be the main instruction set. This could be for additionall functions, defniitions and overall integration.  This manner of promming is very similar to writing a python script. It encapsulates functionalities into distinct, reusable modules. Except here they are called ".md files". After storing all of the md files that define all of the functions I want the gpt to do I leverage the main instruction set or "instructions.md" as the, main logic section and sequentially call all of the functions

# --- // PROMPT 3 // ========

To ensure the files you've provided fit seamlessly into the well-explained prompt engineering project for "The Assimilator," let's align the main instruction set and the additional `.md` files for optimal integration and functionality. Here's how to proceed:

1.  **Main Instruction Set (`Instructions.md`)**: This file serves as the central logic unit, orchestrating the flow of operations and integrating functionalities defined in the markdown (.md) files. It specifies constants, definitions, functions, and workflows, guiding the AI's behavior in processing inputs and generating outputs.

2.  **Integrating `.md` Files**:

    *   Each `.md` file, such as `menu.md`, `file_handling.md`, and `filesaving.config`, encapsulates specific functionalities or information, analogous to modules in a Python script.
    *   The `file_read` function defined in `Instructions.md` is pivotal for dynamically loading and executing the logic contained within these `.md` files, allowing for modularity and expandability.
3.  **Execution Flow**:

    *   Upon receiving a command, "The Assimilator" references the `PRIMARY_PATH` to locate and read the relevant `.md` file, parsing its content to execute the defined instructions.
    *   This modularity ensures that adjustments or expansions, such as adding new functionalities or refining existing ones, only necessitate modifications to the specific `.md` files without altering the core logic in `Instructions.md`.
4.  **Forward Compatibility**:

    *   By maintaining a structured and modular approach, any future updates or enhancements can be seamlessly integrated by adding or modifying the `.md` files, ensuring backward compatibility and minimizing disruptions to the existing workflow.
5.  **Integration Example**:

    *   To integrate a new function, simply add a corresponding `.md` file defining the function's logic. Update `Instructions.md` to include a reference to this new file under the appropriate section (e.g., Workflow Management).
    *   Use the `file_read` function to dynamically load and apply the new logic during the system's operation, ensuring that "The Assimilator" remains adaptable and scalable.

This approach emphasizes the inherent modularity and expandability of "The Assimilator," aligning with the concept of programming GPT in a fashion akin to scripting, where `.md` files serve as distinct, reusable modules.
